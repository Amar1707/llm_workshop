{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d291f722",
   "metadata": {},
   "source": [
    "# The NLP Jounrney\n",
    "<img src=\"assets/banner_notebook_1.jpg\">\n",
    "\n",
    "\n",
    "The NLP domain wasn't always this buzzing with __attention__ and hype that we see today. \n",
    "The recent progress in this field is built on top of years of amazing work and research. Before we leap onto the current state of things, let us have a quick walk through of how we arrived here. The current NLP systems are standing tall and promising on the shoulders of very solid work from past decades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a8fef",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop_dhs23/blob/main/module_01/solutions/nlp_journey.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560dd911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data, models and config folder\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"config\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559da045",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a55377",
   "metadata": {},
   "source": [
    "### Get Text\n",
    "__The Gutenberg Project__ is an amazing project aimed at providing free access to some of the world's most amazing classical works. This makes it a wonderful source of textual data for NLP practitionars to use and improve their understanding of textual data. Ofcourse you can improve your litrary skills too \n",
    "\n",
    "For this module and workshop in general we will make use of materials available from the project. We begin by downloading the book __\"The Adventures of Sherlock Holmes by Arthur Conan Doyle\"__\n",
    "\n",
    "\n",
    "<img src=\"assets/img_2_notebook_1.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037714ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data/sherlock_homes.txt http://www.gutenberg.org/files/1661/1661-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc6a645",
   "metadata": {
    "id": "RrwoWo-Yon-9"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487b236",
   "metadata": {
    "id": "-lKQYgNXonkD"
   },
   "outputs": [],
   "source": [
    "filename = DATA_DIR+\"sherlock_homes.txt\"\n",
    "file_text = open(filename, 'r', encoding='utf-8').read()\n",
    "\n",
    "# lower case text to reduce dimensionality\n",
    "file_text = file_text.lower()\n",
    "\n",
    "# We remove first 1450 characters to remove\n",
    "# details related to project gutenberg\n",
    "raw_text = file_text [1450:]\n",
    "\n",
    "print(\"Total characters =\", len(raw_text))\n",
    "print(\"Unique characters =\", len(set(raw_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bdf7bf",
   "metadata": {},
   "source": [
    "### Text Representation\n",
    "\n",
    "Feature Engineering is often known as the secret sauce to creating superior and better performing machine learning models. Just one excellent feature could be your ticket to winning a Kaggle challenge! The importance of feature engineering is even more important for unstructured, textual data because we need to convert free flowing text into some numeric representations which can then be understood by machine learning algorithms.\n",
    "\n",
    "Since text is mostly available in unstructured form yet very high in dimensionality (how??? :sweat: ), the ability to represent text in the most appropriate way is one of the key ingredients to work in this domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eabc54",
   "metadata": {},
   "source": [
    "### Tokenize and Vectorize\n",
    "To leverage different algorithms we convert text into numbers that can be represented as tensors.\n",
    "\n",
    "The first step is to convert text to tokens - tokenization. If we use word-level representation, each word would be represented by its own token. We will use built-in tokenizer from torchtext module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7dcc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2736a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(raw_text[:50])\n",
    "print(f'Token list:\\n{tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1e369",
   "metadata": {},
   "source": [
    "Now, to convert text to numbers, we will need to build a vocabulary of all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level vocab\n",
    "word_counter = collections.Counter()\n",
    "for line in raw_text.split('\\n'):\n",
    "    word_counter.update(tokenizer(line))\n",
    "word_vocab = torchtext.vocab.Vocab(word_counter)\n",
    "word_vocab_size = len(word_vocab)\n",
    "print(\"word vocab size =\", word_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample lookup at word-level\n",
    "tokens = tokenizer(raw_text[:50])\n",
    "word_lookup = [list((word_vocab[w], w)) for w in tokens]\n",
    "print(\"Index lookup of words in sample sentence:\",\"\\n\",word_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92c695",
   "metadata": {},
   "source": [
    "### Text as Vector\n",
    "\n",
    "``torchtext`` ``vocab.stoi`` dictionary allows us to convert from a string representation into numbers (``stoi`` -> \"from string to integers).\n",
    "\n",
    "To convert the text back from a numeric representation into text, we can use the ``vocab.itos`` dictionary to perform reverse lookup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aadd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return [word_vocab[s] for s in tokenizer(x)]\n",
    "\n",
    "vec = encode(raw_text[:100])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb01e9",
   "metadata": {},
   "source": [
    "### Bag Of Words Representation\n",
    "\n",
    "Bag of Words (BoW) representation is a traditional vector representation of text for NLP tasks. Each word/character is linked to a vector index, vector element contains the number of occurrences of a word/character in a given document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614878a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bow(text,bow_vocab_size=word_vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed150909",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"this is a sample text to showcase text representation\"\n",
    "vec = to_bow(sample_text)\n",
    "print(\"BoW vector:\",\"\\n\",vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec[word_vocab[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab['<unk>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebc9a6",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF-IDF stands for term frequency–inverse document frequency. It is a form of bag of words representation, where instead of a **binary value** indicating the appearance of a word in a document, a floating-point value is used, which is related to the frequency of word occurrence in the corpus.\n",
    "\n",
    "The formula to calculate TF-IDF is:\n",
    "\n",
    "$w_{ij}=tf_{ij}* \\log(\\frac{N}{df_i})$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $i$ is the word\n",
    "- $j$ is the document\n",
    "- $w_{ij}$ is the weight or the importance of the word in the document\n",
    "- $tf_{ij}$ is the number of occurrences of the word i in the document j, i.e. the BoW value we have seen before\n",
    "- $N$ is the number of documents in the collection\n",
    "- $df_i$ is the number of documents containing the word i in the whole collection.\n",
    "\n",
    "\n",
    "TF-IDF value $w_{ij}$ increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contains the word, which helps to adjust for the fact that some words appear more frequently than others. For example, if the word appears in every document in the collection, $df_i=N$, and $w_{ij}=0$, and those terms would be completely disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9278e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_lines = raw_text.split('\\n')\n",
    "raw_text_lines = [line for line in raw_text_lines if line not in [' ','']]\n",
    "print(\"total lines =\", len(raw_text_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_lines[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(raw_text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44557c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[3][0,vectorizer.vocabulary_[\"eclipses\"]],\n",
    "      X[3][0,vectorizer.vocabulary_[\"and\"]],\n",
    "      X[3][0,vectorizer.vocabulary_[\"eyes\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222e167",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "A word embedding is a learned dense representation of text. In this approach we represent words and documents as dense vectors that have distinct lexical properties. This can be considered as one of the key breakthroughs in the fielf of NLP.\n",
    "\n",
    "Let us briefly:\n",
    "\n",
    "- Understand the Word2Vec models called Skipgram and CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3601e8",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "This model was created by [Mikolov et. al at Google in 2013](https://arxiv.org/abs/1301.3781). It is a predictive deep learning model designed to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity. Essentially these are unsupervised models which can be trained on massive textual corpora, create a vocabulary of possible words and generate dense word embeddings for each word in the vector space representing that vocabulary.\n",
    "\n",
    "There are two different model architectures which can be leveraged by Word2Vec to create these word embedding representations. These include,\n",
    "\n",
    "- The Continuous Bag of Words (CBOW) Model\n",
    "- The Skip-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd8c9b",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (CBOW) Model\n",
    "The CBOW model architecture tries to predict the current __`target word`__ (the center word) based on the __`source context words`__ (surrounding words).\n",
    "\n",
    "Considering a simple sentence, “the quick brown fox jumps over the lazy dog”, this can be pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on.\n",
    "\n",
    "Thus the model tries to predict the target_word based on the context_window words.\n",
    "\n",
    "<img src=\"assets/cbow_arch_notebook_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53e33f",
   "metadata": {},
   "source": [
    "### Skip-gram Model\n",
    "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the __`source context words`__ (surrounding words) given a __`target word`__ (the center word).\n",
    "\n",
    "Considering our simple sentence from earlier, “the quick brown fox jumps over the lazy dog”. If we used the CBOW model, we get pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on.\n",
    "\n",
    "Now considering that the skip-gram model’s aim is to predict the context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word from its target word. Hence the task becomes to predict the context [quick, fox] given target word ‘brown’ or [the, brown] given target word ‘quick’ and so on.\n",
    "\n",
    "Thus the model tries to predict the context_window words based on the target_word.\n",
    "\n",
    "<img src=\"assets/skipgram_arch_notebook_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163092d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab1f11",
   "metadata": {},
   "source": [
    "## Gensim Framework\n",
    "\n",
    "The ``gensim`` framework, created by Radim Řehůřek consists of a robust, efficient and scalable implementation of the __Word2Vec__ model. We will leverage the same on our sample toy corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it.\n",
    "\n",
    "- vector_size: The word embedding dimensionality\n",
    "- window: The context window size\n",
    "- min_count: The minimum word count\n",
    "- sample: The downsample setting for frequent words\n",
    "- sg: Training model, 1 for skip-gram otherwise CBOW\n",
    "\n",
    "We will build a simple Word2Vec model on the corpus and visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [nltk.word_tokenize(line) for line in norm_corpus]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality\n",
    "window_context = 5   # Context window size\n",
    "min_word_count = 1   # Minimum word count\n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus,\n",
    "                              vector_size=feature_size,\n",
    "                              window=window_context,\n",
    "                              min_count = min_word_count,\n",
    "                              sg=sg,\n",
    "                              sample=sample,\n",
    "                              epochs=5000)\n",
    "w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['sky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c28072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scienceplots\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "plt.style.use(['science','ieee','no-latex'])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embeddings\n",
    "words = w2v_model.wv.index_to_key\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1],)\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c23f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('dog', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835132c",
   "metadata": {
    "id": "wfnq6B2K4RV2"
   },
   "source": [
    "## Similar and Improved Works \n",
    "- [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- [FastText](https://arxiv.org/pdf/1607.04606.pdf)\n",
    "- [Sent2Vec](https://arxiv.org/abs/1405.4053)\n",
    "- X2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b8cfc7",
   "metadata": {
    "id": "HHRdBNxQ5Ho6"
   },
   "source": [
    "### Limitations\n",
    "One key limitation of traditional pretrained embedding representations such as Word2Vec is the problem of word sense and removing ambiguity by making them clear. While pretrained embeddings can capture some of the meaning of words in context, every possible meaning of a word is encoded into the same embedding. This can cause problems in downstream models, since many words such as the word 'play' have different meanings depending on the context they are used in.\n",
    "\n",
    "For example, the word 'play' in these two different sentences have quite different meaning:\n",
    "\n",
    "- I went to a **play** at the theatre.\n",
    "- John wants to **play** with his friends.\n",
    "The pretrained embeddings above represent both meanings of the word 'play' in the same embedding. To overcome this limitation, we need to build embeddings based on the language model, which is trained on a large corpus of text, and knows how words can be put together in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc9bb6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e601114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
