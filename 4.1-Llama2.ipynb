{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1378d7",
   "metadata": {
    "id": "KfJ4nkINvlan"
   },
   "source": [
    "# Prompt Engineering\n",
    "<img src=\"assets/pe_banner.jpg\">\n",
    "\n",
    "Prompt Engineering is this thrilling new discipline that opens the door to a world of possibilities with large language models (LLMs).\n",
    "\n",
    "As a prompt engineer, you'll delve into the depths of LLMs, unraveling their capabilities and limitations with finesse. But prompt engineering isn't about mere prompts. It is aa combination of skills and techniques, enabling you to interact and innovate through the use of LLMs.\n",
    "\n",
    "In this module, we will step into the fascinating world of prompt engineering, where we will learn about key principals of working with LLMs through prompts.\n",
    "\n",
    "## Local Model using GPT4ALL\n",
    "> GPT4All is an open-source software ecosystem that allows anyone to train and deploy powerful and customized large language models (LLMs) on everyday hardware. Nomic AI oversees contributions to the open-source ecosystem ensuring quality, security and maintainability.\n",
    "\n",
    "It provides easy to setup and use python bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8e193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gpt4all.GPT4All(\"llama-2-7b-chat.ggmlv3.q4_0.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e6f48",
   "metadata": {
    "id": "2MejaM6ov_jG"
   },
   "source": [
    "## Prompting Basics\n",
    "\n",
    "+ Be Clear and Provide Specific Instructions\n",
    "+ Allow Time to **Think**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Sky is \"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69294b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"[INST] Sky is [/INST]\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Complete the sentence.\n",
    "Sky is \"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"[INST] Complete the sentence.\n",
    "Sky is [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Complete the sentence. Provide few alternatives.\n",
    "Sky is \"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"[INST] Complete the sentence. Provide few alternatives.\n",
    "Sky is [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be Clear and Specific\n",
    "\n",
    "# Example: Clearly state which text to look at, provide delimiters\n",
    "text = \"\"\"\n",
    "The dominant sequence transduction models are based on complex recurrent or \n",
    "convolutional neural networks in an encoder-decoder configuration. The best \n",
    "performing models also connect the encoder and decoder through an attention \n",
    "mechanism. We propose a new simple network architecture, the Transformer, \n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "Experiments on two machine translation tasks show these models to be superior in quality \n",
    "while being more parallelizable and requiring significantly less time to train.\n",
    "\"\"\"\n",
    "\n",
    "prompt=f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, \\\n",
    "toxic, dangerous, or illegal content. \\\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \\\n",
    "If a question does not make any sense, or is not factually coherent, \\\n",
    "explain why instead of answering something not correct. \\\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Summarize the text wrapped in <begin> and <end> tags in a sentence. Identify key contributions.\n",
    "<begin> {text} <end> [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be Clear and Specific\n",
    "text = \"\"\"\n",
    "The dominant sequence transduction models are based on complex recurrent or \n",
    "convolutional neural networks in an encoder-decoder configuration. The best \n",
    "performing models also connect the encoder and decoder through an attention \n",
    "mechanism. We propose a new simple network architecture, the Transformer, \n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "Experiments on two machine translation tasks show these models to be superior in quality \n",
    "while being more parallelizable and requiring significantly less time to train.\n",
    "\"\"\"\n",
    "\n",
    "prompt=f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, \\\n",
    "toxic, dangerous, or illegal content. \\\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \\\n",
    "If a question does not make any sense, or is not factually coherent, \\\n",
    "explain why instead of answering something not correct. \\\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Summarize the text wrapped in <begin> and <end> tags in only a sentence. Provide a title for the summary.\n",
    "<begin> {text} <end> [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c77cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define her behavior/role\n",
    "prompt=\"\"\"[INST] <<SYS>>\n",
    "You are Donald Trump. You are being interviewed. Keep your answers very short.\n",
    "<</SYS>>\n",
    "\n",
    "What is your opinion about Joe Biden? [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"[INST] <<SYS>>\n",
    "You are Donald Trump. You are being interviewed. Keep your answers very short.\n",
    "<</SYS>>\n",
    "\n",
    "What do you think about your loss against Joe Biden? [/INST]\"\"\"\n",
    "response = model.generate(prompt, max_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2cede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"[INST] <<SYS>>\n",
    "You are Donald Trump. You are being interviewed. Keep your answers very short.\n",
    "<</SYS>>\n",
    "\n",
    "You lost the election. So, what next? [/INST]\"\"\"\n",
    "response = model.generate(prompt, max_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"[INST] <<SYS>>\n",
    "You are a parental counselor. Your job is to guide parents in dealing with their kids.\n",
    "<</SYS>>\n",
    "\n",
    "Hi doctor! I am Sushma. My daughter, Priya was caught stealing crayons of her friend in the school. \\\n",
    "She is just 5 years old. I am so angry, I feel like slapping her. What should I do? [/INST]\"\"\"\n",
    "response = model.generate(prompt, max_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e71912",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"[INST] <<SYS>>\n",
    "You are a career counselor. Your job is help people in their professional lives.\n",
    "<</SYS>>\n",
    "\n",
    "Hi doctor! I am Amar. I am confused between Computer Science and Electronics. What is your recommendation? \\\n",
    "Which one of these two has better job prospects? [/INST]\"\"\"\n",
    "response = model.generate(prompt, max_tokens=1000)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a228fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be Clear and Specific, aka provide step by step instructions\n",
    "text = \"\"\"\n",
    "To make tea you first need to have a cup full of water,\n",
    "half cup milk, some sugar and tea leaves. Start by boiling water.\n",
    "Once it comes to a boil, add milk to it. Next step is to add tea and\n",
    "let it boil for another minute.\n",
    "Add sugar to taste. Serve in a tall glass.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, \\\n",
    "toxic, dangerous, or illegal content. \\\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \\\n",
    "If a question does not make any sense, or is not factually coherent, \\\n",
    "explain why instead of answering something not correct. \\\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Read the text wrapped between <begin> and <end> tags.\n",
    "Check if it contains a sequence of instructions, \\\n",
    "re-write the instructions in the following format:\n",
    "\n",
    "Point 1 - ...\n",
    "Point 2 - ...\n",
    "...\n",
    "Point N - ...\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\\n",
    "then apologize that you cannot rephrase such text.\n",
    "\n",
    "<begin> {text} <end> [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without instructions\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, \\\n",
    "toxic, dangerous, or illegal content. \\\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \\\n",
    "If a question does not make any sense, or is not factually coherent, \\\n",
    "explain why instead of answering something not correct. \\\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "What are snakes? [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without instructions\n",
    "prompt = f\"\"\"[INST] What are snakes? [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be Clear and Specific, aka provide examples\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "Your task is to answer in conversation style mentioned in <begin> and <end> tags.\n",
    "Keep answers very short similar to examples provided in the text.\n",
    "<begin>\n",
    "<kid>: What are birds?\n",
    "<father>: birds are cute little creatures that can fly\n",
    "\n",
    "<kid>: What are whales?\n",
    "<father>: Whales are very big fish that roam the oceans\n",
    "<end>\n",
    "<</SYS>>\n",
    "\n",
    "<kid>: What are snakes? [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7018cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow for time to think (similar to step by step instructions)\n",
    "text = \"\"\"\n",
    "Our last holiday was in Karnataka. We visited Hampi, Gokarna and Coorg.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, \\\n",
    "toxic, dangerous, or illegal content. \\\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \\\n",
    "If a question does not make any sense, or is not factually coherent, \\\n",
    "explain why instead of answering something not correct. \\\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Summarize the text mentioned in <begin> and <end> tags briefly. \\\n",
    "Then follow the instructions :\n",
    "1 - Translate the summary to Hindi.\n",
    "2 - List each city in the text.\n",
    "3 - Output a python dictionary that contains the following \\\n",
    "keys: original_text, hindi_translation, kannada_translation, tamil_translation, num_cities, city_names.\n",
    "\n",
    "<begin> {text} <end> [/INST]\"\"\"\n",
    "response = model.generate(prompt, max_tokens=700)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow time to think, aka ask LLM to generate its own answer and then compare\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, \\\n",
    "toxic, dangerous, or illegal content. \\\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \\\n",
    "If a question does not make any sense, or is not factually coherent, \\\n",
    "explain why instead of answering something not correct. \\\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Question is mentioned in <question-begin> and <question-end> tags. \\\n",
    "User's solution is mentioned in <solution-begin> and <solution-end> tags. \\\n",
    "Determine if the user's solution is correct or not.\n",
    "To complete this task the instructions are as follows:\n",
    "- Step 1: prepare your own solution step by step to the question.\n",
    "- Step 2: Compare your solution to the user's solution \\\n",
    "and evaluate if the user's solution is correct or not.\n",
    "Do not decide if the solution is correct until you have solved the question yourself.\n",
    "<question-begin>\n",
    "I went to the market and bought 10 apples.\n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "<question-end>\n",
    "<solution-begin>\n",
    "1. I started with 10 apples.\n",
    "2. I gave away 2 apples to the neighbor and 2 to the repairman, so now I have 6 apples left.\n",
    "3. Then I bought 5 more apples, so now I have 11 apples.\n",
    "4. I then ate 1 apple, so I will have only 10 apples with me.\n",
    "<solution-end>\n",
    "Use the following format for your response:\n",
    "Question:```question here```\n",
    "User's solution:```user's solution here```\n",
    "Actual solution:```your step by step solution here```\n",
    "Is the user's solution the same as actual solution \\\n",
    "just calculated:```yes or no with reason```\n",
    "Final Answer:```correct or incorrect``` [/INST]\"\"\"\n",
    "response = model.generate(prompt, max_tokens=700)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949557a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot prompting\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, \\\n",
    "toxic, dangerous, or illegal content. \\\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \\\n",
    "If a question does not make any sense, or is not factually coherent, \\\n",
    "explain why instead of answering something not correct. \\\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Classify the sentiment of the given text as neutral, positive or negative.\n",
    "Text: The food at this restaurant is so bad.\n",
    "Sentiment: [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822401a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few shot prompting\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible, while being safe.  \\\n",
    "Your answers should not include any harmful, unethical, racist, sexist, \\\n",
    "toxic, dangerous, or illegal content. \\\n",
    "Please ensure that your responses are socially unbiased and positive in nature. \\\n",
    "If a question does not make any sense, or is not factually coherent, \\\n",
    "explain why instead of answering something not correct. \\\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Given a sentence, classify it's sentiment as neutral, positive or negative. Below are a few examples.\n",
    "\n",
    "Example 1:\n",
    "Input: Superb drinks and amazing service!\n",
    "Output: Positive\n",
    "\n",
    "Example 2:\n",
    "Input: I don't understand why this place is so expensive, worst food ever.\n",
    "Output: Negative\n",
    "\n",
    "Example 3:\n",
    "Input: Totally worth it, tasty 100%.\n",
    "Output: Positive\n",
    "\n",
    "Classify the sentiment of the following Input sentence. \\\n",
    "Respond with either of the three words: positive, negative or neutral.\n",
    "Note: you are to output the sentiment after \"Output: \". Do not include \"Output: \" in your answer.\n",
    "Input: This place is such an utter waste of time.\n",
    "Output: [/INST]\"\"\"\n",
    "response = model.generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b212cf",
   "metadata": {},
   "source": [
    "### Chain of Thought (COT)\n",
    "Tasks which are more complex and require a bit of *reasoning* (careful there ðŸ˜‰ ) require special measures. Introduced by in a paper of similar title by [Wei et. al.](https://arxiv.org/abs/2201.11903) combines few-shot prompting with additional instructions for the LLM to think through while generating the response.\n",
    "\n",
    "_Sample Prompt_:\n",
    "<img src=\"assets/cot_few_shot.png\">\n",
    "\n",
    "> Source: [Wei et. al.](https://arxiv.org/abs/2201.11903)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94b7da",
   "metadata": {},
   "source": [
    "## Advanced Prompting Techniques\n",
    "Prompt Engineering or PE is an active area of research where new techniques\n",
    "are being explored every day. Some of these are:\n",
    "\n",
    "  - [Auto Chain of Thought](https://arxiv.org/abs/2210.03493)\n",
    "  - [Majority Vote or Self-Consistency](https://arxiv.org/abs/2203.11171)\n",
    "  - [Tree of Thoughts](https://arxiv.org/abs/2305.10601)\n",
    "  - Augmented Generation/Retrieval\n",
    "  - [Auto Prompt Engineering (APE)](https://arxiv.org/abs/2211.01910)\n",
    "  - [Multi-modal Prompting](https://arxiv.org/abs/2302.00923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cefdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[\"hello\",\"write me a short poem\",\"thank you for the beautiful poem\",\"who is the author of this poem?\",\n",
    "         \"summarize the poem in a sentence\",\"what is the first line of the poem?\",\n",
    "         \"what is the last line of the poem?\",\"see you later\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba152b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with model.chat_session():\n",
    "    for prompt in prompts:\n",
    "        response = model.generate(prompt)\n",
    "    for message in model.current_chat_session:\n",
    "        print(\"***** \",message[\"role\"], \" *****\")\n",
    "        content = \"[INST] \" + message[\"content\"] + \" [/INST]\" if message[\"role\"]==\"user\" else message[\"content\"]\n",
    "        print(content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b2821",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages=[{\"role\":\"system\", \"content\":model.config[\"systemPrompt\"]}]\n",
    "prompt=model.config[\"systemPrompt\"]\n",
    "for i, p in enumerate(prompts):\n",
    "    content=model.config[\"promptTemplate\"].format(p)\n",
    "    prompt += \"\\n\" + content\n",
    "    messages.append({\"role\":\"user\", \"content\":p})\n",
    "    response = model.generate(prompt)\n",
    "    messages.append({\"role\":\"assistant\", \"content\":response})\n",
    "    print(\"**********\", \"prompt\", \"**********\")\n",
    "    print(prompt)\n",
    "    print(\"=======\", \"response\", \"=======\")\n",
    "    print(response,\"\\n\\n\")\n",
    "    prompt += \"\\n\" + response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af4315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
